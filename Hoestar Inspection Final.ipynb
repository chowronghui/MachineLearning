{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyod'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5ef97a50b3df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend_pdf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPdfPages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIsolationForest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhbos\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHBOS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyod'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statistics \n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from pyod.models.hbos import HBOS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "# raw_data= pd.read_csv(r\"C:\\Users\\Swati Gupta\\Desktop\\PD data.csv\")\n",
    "raw_data= pd.read_csv(r\"PD data.csv\")\n",
    "print(raw_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To detect outliers by different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-score method\n",
    "def outliers(raw_data,column):\n",
    "    Average= np.mean(raw_data[column])\n",
    "    Std_dev= np.std(raw_data[column])\n",
    "    outliers_value= []\n",
    "    for i in raw_data[column]:\n",
    "        z_score= (i-Average)/Std_dev\n",
    "        if z_score >3 or z_score <-3:\n",
    "            outliers_value.append(i)\n",
    "    distribution_plot = stats.norm.pdf(raw_data[column], Average, Std_dev) \n",
    "    pl.plot(raw_data[column],distribution_plot, color='coral')\n",
    "    pl.show()\n",
    "    return sorted(outliers_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PD Count outlier values: \",outliers(raw_data,'PD Count'))\n",
    "print(\"PD Average outlier values: \",outliers(raw_data,'PD Average'))\n",
    "print(\"Temperature outlier values: \",outliers(raw_data,'Temperature'))\n",
    "print(\"Humidity outlier values: \",outliers(raw_data,'Humidity'))\n",
    "print(\"Loading outlier values: \",outliers(raw_data,'Loading'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Discover outliers with boxplot\n",
    "\n",
    "sns.boxplot(x=raw_data['PD Count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=raw_data['PD Average'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=raw_data['Temperature'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(raw_data['Humidity'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=raw_data['Loading'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_by_iqr(raw_data,column):\n",
    "    sorted(raw_data)\n",
    "    Q1, Q3= np.percentile(raw_data[column],[25,75])\n",
    "    outliers_value= []\n",
    "    for i in raw_data[column]:\n",
    "        IQR= Q3 - Q1\n",
    "        if (i < (Q1 - 1.5 * IQR)) |(i > (Q3 + 1.5 * IQR)):\n",
    "            outliers_value.append(i)\n",
    "    return sorted(outliers_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"PD Average outlier values by IQR: \",outliers_by_iqr(raw_data,'PD Average'))\n",
    "print(\"PD Count outlier values by IQR: \",outliers_by_iqr(raw_data,'PD Count'))\n",
    "print(\"Temperature outlier values by IQR: \",outliers_by_iqr(raw_data,'Temperature'))\n",
    "print(\"Humidity outlier values by IQR: \",outliers_by_iqr(raw_data,'Humidity'))\n",
    "print(\"Loading outlier values by IQR: \",outliers_by_iqr(raw_data,'Loading'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RELATIONSHIP BETWEEN FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TO CHECK THE RELATIONSHIP BETWEEN FEATURES\n",
    "svm=sns.pairplot(raw_data, kind=\"scatter\")\n",
    "svm.savefig('image2.png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pearson_heat_map(raw_data):\n",
    "    pearsoncorr = pd.DataFrame(raw_data).corr(method='pearson')\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(pearsoncorr, \n",
    "                xticklabels=pearsoncorr.columns,\n",
    "                yticklabels=pearsoncorr.columns,\n",
    "                cmap='RdBu_r',\n",
    "                annot=True,\n",
    "                linewidth=1)\n",
    "    \n",
    "find_pearson_heat_map(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find out the relationship BY KENDALL\n",
    "x1= raw_data['Humidity']\n",
    "x2= raw_data['Temperature']\n",
    "kendal_ration,p_value= stats.kendalltau(x1,x2)\n",
    "print(kendal_ration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anova correleation()\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "def get_anova(raw_data, column):\n",
    "    \n",
    "    avg_data_map = {}\n",
    "    base_col = 'Equipment ID'\n",
    "    for index, row in  raw_data.iterrows():\n",
    "        if row[base_col] in avg_data_map:\n",
    "            avg_data_map[int(row[base_col])].append(row[column])\n",
    "\n",
    "        else:\n",
    "            avg_data_map[int(row[base_col])] = [row[column]]\n",
    "    avg_data_lst = []\n",
    "    for key in avg_data_map:\n",
    "        avg_data_lst.append (avg_data_map[key])\n",
    "    return f_oneway(*avg_data_lst)\n",
    "    \n",
    "print('score of PD-Average is: '+  str(get_anova(raw_data,'PD Average')))\n",
    "print('score of PD-Count is: '+  str(get_anova(raw_data,'PD Count')))\n",
    "print('score of Temperature is: '+  str(get_anova(raw_data,'Temperature')))\n",
    "print('score of Humidity is: '+  str(get_anova(raw_data,'Humidity')))\n",
    "print('score of Loding is: '+  str(get_anova(raw_data,'Loading')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalies Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_value(raw_data):\n",
    "    columns_needs_standardize= ['PD Average', 'Humidity', 'Temperature', 'Loading', 'PD Count']\n",
    "    data= raw_data[columns_needs_standardize] \n",
    "    X = StandardScaler().fit_transform(data)\n",
    "    return X\n",
    "\n",
    "def plot_cluster(data, pred_clusters, column1, column2, title):\n",
    "    data['clusters'] = pred_clusters\n",
    "    clusters = {}\n",
    "    for label in pred_clusters:\n",
    "        if label not in clusters:\n",
    "            clusters[label] = data[data.clusters == label]\n",
    "\n",
    "    for key in clusters:\n",
    "        plt.scatter(clusters[key][column1],clusters[key][column2])\n",
    "\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def start_plotting(data, y_train):\n",
    "    plot_cluster(data, y_train, 'PD Average', 'PD Count', 'PD Average with PD Count')  \n",
    "    plot_cluster(data, y_train, 'PD Average', 'Temperature', 'PD Average with Temperature')\n",
    "    plot_cluster(data, y_train, 'PD Average', 'Humidity', 'PD Average with Humidity')\n",
    "    plot_cluster(data, y_train, 'PD Average', 'Loading', 'PD Average with Loading')\n",
    "    plot_cluster(data, y_train, 'PD Count', 'Temperature', 'PD Count with Temperature')\n",
    "    plot_cluster(data, y_train, 'PD Count', 'Humidity', 'PD Count with Humidity')\n",
    "    plot_cluster(data, y_train, 'PD Count', 'Loading', 'PD Count with Loading')\n",
    "    plot_cluster(data, y_train, 'Temperature', 'Loading', 'Temperature with Loading')\n",
    "    plot_cluster(data, y_train, 'Temperature', 'Humidity', 'Temperature with Humidity')\n",
    "    plot_cluster(data, y_train, 'Loading', 'Humidity', 'Loading with Humidity')\n",
    "    \n",
    "def k_means():\n",
    "    k_means = KMeans(n_clusters=3, random_state=10,max_iter=1000) \n",
    "    X = get_scaled_value(raw_data)\n",
    "    y_predict = k_means.fit_predict(X)\n",
    "    start_plotting(raw_data, y_predict)\n",
    "    \n",
    "# def db_scan():\n",
    "#     from sklearn.cluster import DBSCAN\n",
    "#     X = get_scaled_value(raw_data)\n",
    "#     clustering = DBSCAN().fit(X)\n",
    "#     start_plotting(raw_data, clustering.labels_)\n",
    "    \n",
    "def isolation_forest():\n",
    "    X = get_scaled_value(raw_data)\n",
    "    clf = IsolationForest(random_state=24).fit(X)\n",
    "    X = get_scaled_value(raw_data)\n",
    "    y_train=clf.predict(X)\n",
    "    start_plotting(raw_data, y_train)\n",
    "    \n",
    "def hbos():\n",
    "    classifier = HBOS(contamination=0.05)\n",
    "    X = get_scaled_value(raw_data)\n",
    "    clf=classifier.fit(X)\n",
    "    y_train=clf.predict(X)\n",
    "    start_plotting(raw_data, y_train)\n",
    "\n",
    "def agglomerative():\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    X = get_scaled_value(raw_data)\n",
    "\n",
    "    data_scaled = pd.DataFrame(X)\n",
    "    cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')  \n",
    "    y_train=cluster.fit_predict(data_scaled)\n",
    "    data['clusters'] = y_train\n",
    "    start_plotting(data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolation_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
